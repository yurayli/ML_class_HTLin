}
}
Ks <- c(Ks, sum(K))
}
SVs <- c()
Eout <- c()
Ks <- c()
C <- c(.001, .01, .1, 1, 10)
for (i in C) {
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = i, gamma = 100, cross = 5)
pred3 <- predict(model3, x)
SVs <- c(SVs, nrow(model3$SV))
pred <- predict(model, xtest)
Eout <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != ytest)))
K = matrix(rep(0, nrow(x)), nrow(x), nrow(x))
for (j in 1:nrow(x)) {
for (k in 1:nrow(x)) {
K[i,j] = exp(- 100 * t((x[j,] - x[k,])) %*% (x[j,] - x[k,]) )
}
}
Ks <- c(Ks, sum(K))
}
x[1,] - x[1,]
(x[1,] - x[1,]) %*% (x[1,] - x[1,])
(x[1,] - x[1,]) %*% t(x[1,] - x[1,])
t(x[1,] - x[1,]) %*% (x[1,] - x[1,])
class(x[1,] - x[1,])
t(as.matrix(x[1,] - x[1,])) %*% as.matrix(x[1,] - x[1,])
as.matrix(x[1,] - x[1,]) %*% t(as.matrix(x[1,] - x[1,]))
SVs <- c()
Eout <- c()
Ks <- c()
C <- c(.001, .01, .1, 1, 10)
for (i in C) {
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = i, gamma = 100, cross = 5)
pred3 <- predict(model3, x)
SVs <- c(SVs, nrow(model3$SV))
pred <- predict(model, xtest)
Eout <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != ytest)))
K = matrix(rep(0, nrow(x)), nrow(x), nrow(x))
for (j in 1:nrow(x)) {
for (k in 1:nrow(x)) {
K[i,j] = exp(- 100 * as.matrix((x[j,] - x[k,])) %*% t(as.matrix(x[j,] - x[k,])) )
}
}
Ks <- c(Ks, sum(K))
}
SVs <- c()
Eout <- c()
Ks <- c()
C <- c(.001, .01, .1, 1, 10)
for (i in C) {
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = i, gamma = 100, cross = 5)
SVs <- c(SVs, nrow(model3$SV))
pred <- predict(model3, xtest)
Eout <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != ytest)))
K = matrix(rep(0, nrow(x)), nrow(x), nrow(x))
for (j in 1:nrow(x)) {
for (k in 1:nrow(x)) {
K[i,j] = exp(- 100 * as.matrix((x[j,] - x[k,])) %*% t(as.matrix(x[j,] - x[k,])) )
}
}
Ks <- c(Ks, sum(K))
}
SVs <- c()
Eout <- c()
Ks <- c()
C <- c(.001, .01, .1, 1, 10)
for (i in C) {
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = i, gamma = 100, cross = 5)
SVs <- c(SVs, nrow(model3$SV))
pred <- predict(model3, xtest)
Eout <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != ytest)))
K = matrix(rep(0, nrow(x)), nrow(x), nrow(x))
for (j in 1:nrow(x)) {
for (k in 1:nrow(x)) {
K[j,k] = exp(- 100 * as.matrix((x[j,] - x[k,])) %*% t(as.matrix(x[j,] - x[k,])) )
}
}
Ks <- c(Ks, sum(K))
}
SVs <- c()
Eout <- c()
C <- c(.001, .01, .1, 1, 10)
for (i in C) {
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = i, gamma = 100, cross = 5)
SVs <- c(SVs, nrow(model3$SV))
pred <- predict(model3, xtest)
Eout <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != ytest)))
}
Eout
SVs
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = .001, gamma = 100, cross = 5)
rm(model2)
head(model3$SV)
model3$coefs[1]
model3$coefs[2]
head(model3$coefs, 15)
SVs <- c()
Eout <- c()
Gamma <- c(1, 10, 100, 1000)
for (i in Gamma) {
model3 <- svm(x = x, y = y, type = 'C-classification', kernel = 'radial',
scale = F, cost = 0.1, gamma = i, cross = 5)
SVs <- c(SVs, nrow(model3$SV))
pred <- predict(model3, xtest)
Eout <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != ytest)))
}
Eout
sample(100,5)
sample(100,5)
sample(100,5)
sample(100,5)
sample(10,5)
idx <- sample(10,5)
idx
idx <- sample(10,3)
idx
order(idx)
sort(idx)
arr <- 1:10
arr[arr == idx]
arr %in% idx
arr[! arr %in% idx]
valIndex <- sample(nrow(x), 1000)
xval <- x[valIndex, ]
xtrain <- x[(! 1:nrow(x) %in% valIndex), ]
xtrain <- x[-valIndex, ]
arr[-idx]
rm(y2,y4,y6,y8)
Gamma <- c(1, 10, 100, 1000, 10000)
bestGamms <- c()
for (i in 1:100) {
valIndex <- sample(nrow(x), 1000)
xval <- x[valIndex, ]
yval <- y[valIndex]
xtrain <- x[-valIndex, ]
ytrain <- y[-valIndex]
Evals <- c()
for (i in Gamma) {
model3 <- svm(x = xtrain, y = ytrain, type = 'C-classification', kernel = 'radial',
scale = F, cost = 0.1, gamma = i)
pred <- predict(model3, xval)
Eval <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != yval)))
Evals <- c(Evals, Eval)
}
bestGamma <- which[Evals == min(Evals)][1]
bestGamms <- c(bestGamms, bestGamma)
}
bestGamms <- c()
for (i in 1:100) {
valIndex <- sample(nrow(x), 1000)
xval <- x[valIndex, ]
yval <- y[valIndex]
xtrain <- x[-valIndex, ]
ytrain <- y[-valIndex]
Evals <- c()
for (i in Gamma) {
model3 <- svm(x = xtrain, y = ytrain, type = 'C-classification', kernel = 'radial',
scale = F, cost = 0.1, gamma = i)
pred <- predict(model3, xval)
Eval <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != yval)))
Evals <- c(Evals, Eval)
}
bestGamma <- Gamma[which(Evals == min(Evals))[1]]
bestGamms <- c(bestGamms, bestGamma)
}
bestGamms <- c()
for (i in 1:100) {
valIndex <- sample(nrow(x), 1000)
xval <- x[valIndex, ]
yval <- y[valIndex]
xtrain <- x[-valIndex, ]
ytrain <- y[-valIndex]
Evals <- c()
for (i in Gamma) {
model3 <- svm(x = xtrain, y = ytrain, type = 'C-classification', kernel = 'radial',
scale = F, cost = 0.1, gamma = i)
pred <- predict(model3, xval)
Eval <- c(Eout, mean(as.numeric(as.numeric(as.character(pred)) != yval)))
Evals <- c(Evals, Eval)
}
bestGamma <- Gamma[which(Evals == min(Evals))[1]]
bestGamms <- c(bestGamms, bestGamma)
}
bestGamms <- c()
for (i in 1:100) {
valIndex <- sample(nrow(x), 1000)
xval <- x[valIndex, ]
yval <- y[valIndex]
xtrain <- x[-valIndex, ]
ytrain <- y[-valIndex]
Evals <- c()
for (i in Gamma) {
model3 <- svm(x = xtrain, y = ytrain, type = 'C-classification', kernel = 'radial',
scale = F, cost = 0.1, gamma = i)
pred <- predict(model3, xval)
Eval <- mean(as.numeric(as.numeric(as.character(pred)) != yval))
Evals <- c(Evals, Eval)
}
bestGamma <- Gamma[which(Evals == min(Evals))[1]]
bestGamms <- c(bestGamms, bestGamma)
}
rm(Ks)
length(which(bestGamms == 10))
rm(list = ls())
X <- matrix(c(1,0,0,-1,0,0,-2,0,1,-1,0,2,-2,0), 7,2)
y = c(-1,-1,-1,1,1,1,1)
model <- svm(x = X, y = y, scale = F, kernel = 'linear', type = 'C-classification', cost = 1e9)
model <- svm(x = X, y = y, scale = F, kernel = 'linear', type = 'C-classification', cost = 1e6)
model <- svm(x = X, y = y, scale = F, kernel = 'linear', type = 'C-classification', cost = 1e7)
model <- svm(x = X, y = y, scale = F, kernel = 'linear', type = 'C-classification', cost = 1e6)
model
w <- t(model$coefs) %*% model$SV
b <- -model$rho
model <- svm(x = X, y = y, scale = F, kernel = 'polynomial', type = 'C-classification',
cost = 1e6, coef0 = 1, ddegree = 2)
model <- svm(x = X, y = y, scale = F, kernel = 'polynomial', type = 'C-classification',
cost = 1e8, coef0 = 1, ddegree = 2)
w <- t(model$coefs) %*% model$SV
b <- -model$rho
nrow(model$coefs)
ncol(model$coefs)
nrow(model$SV)
ncol(model$SV)
model
model$coefs * y
model$coefs * y[model$index]
model$coefs
model$index
w <- -w
b <- -b
sum(model$coefs * y[model$index])
w
b
15/9
model$coefs
model$SV
SV <- model$SV
x = 1:10
x = 1:.1:10
x = 1:.1:10
x = 1:.1
?seq
seq(1, 5, 0.1)
x = seq(0,1,0.1)
x
x = seq(-6,6,0.1)
library(pracma)
y = sigmoid(x)
plot(x, y, type = 'p')
plot(x, y, type = 'l')
plot(x, y, type = 'l', col = 'green')
plot(x, y, type = 'l', col = 'blue')
plot(x, y, type = 'l', col = 'blue', cex = 2)
plot(x, y, type = 'l', col = 'blue', cex = 5)
plot(x, y, type = 'l', col = 'blue', lwd = 4)
plot(x, y, type = 'l', col = 'blue', lwd = 3)
plot(x, y, type = 'l', col = 'blue', lwd = 3, ylim = c(0,1))
plot(x, y, type = 'l', col = 'blue', lwd = 3, xlim = c(-4,4), ylim = c(0,1))
plot(x, y, type = 'l', col = 'blue', lwd = 3, xlim = c(-6,6), ylim = c(0,1))
180 + 18^2 + 18
x = -6:6
plot(x, 1/(1+exp(-x)),'p')
x = -10:10
plot(x, 1/(1+exp(-x)),'l')
x = 1e9
for (i in 1e6) {
x = x + 1e-6
}
x = x- 1e9
x
X = 1:9
x
X
?as.array
X = as.array(X, dim = c(3,3))
X
X = as.matrix(X, c(3,3))
X
as.matrix()
?as.matrix
X = as.matrix(X, 3,3)
X
X = 1:9
X
X = as.matrix(X, 3,3)
X
X = 1:9
X = matrix(X, 3,3)
X
X[1,]
X[,2]
X[3,2]
X[3]
X[4]
X[7]
X[3][1]
X[3][2]
X[3,2]
seq(10,3)
seq(0,10)
seq(0,10.2)
seq(0,10,.2)
seq(0,10,2)
runif(2*3, -1,1)
runif(2*3, -1,1)
runif(20*30, -1,1)
sample(10,15)
sample(15,10)
sample(100,10)
sample(100,30,T)
sample(100,5,T)
sample(10,5,T)
sample(100,30,T)
cat('hello',3)
cat('hello',3,'world')
getwd()
setwd('./Desktop/ML class NTU/hw8/')
source('MLHW8.R')
source('MLHW8.R')
source('MLHW8.R')
ptm
proc.time()-ptm
# Training parameters
M1 <- 8                                           # d-M1-M2-1 ANN
M2 <- 3
r <- .1
eta <- .01                                        # d-M-1 ANN, eta in c(.001,.01,.1,1,10)
# Backprop algorithm
Eout_arr <- c()
ptm <- proc.time()
for (exp in 1:1) {
# weights randomly (uniformly) initialized
w1 <- runif((d+1)*M1, -r, r)
w1 <- matrix(w1, (d+1), M1)                         # ((d+1)xM1) array
w2 <- runif((M1+1)*M2, -r, r)
w2 <- matrix(w2, (M1+1), M2)                        # ((M1+1)xM2) array
w3 <- runif((M2+1)*1, -r, r)
w3 <- matrix(w3, (M2+1), 1)                         # ((M2+1)x1) array
for (iter in 1:5e4) {
# Step 1: feedforward
hidden1 <- tanh(X %*% w1)
hidden1 <- cbind(rep(1, N), hidden1)                  # (Nx(M1+1)) array
hidden2 <- tanh(hidden1 %*% w2)
hidden2 <- cbind(rep(1, N), hidden2)                  # (Nx(M2+1)) array
H <- tanh(hidden2 %*% w3)                             # hypothesis (Nx1) array
# Step 2: backprop and get the gradient of SQUARED ERROR
delH <- -2 * (y - H) * (1 - H^2)                      # (Nx1) array
delHidden2 <- (delH %*% t(w3)) * (1 - hidden2^2)      # (Nx(M2+1)) array
delHidden1 <- (delHidden2[, 2:(M2+1)] %*% t(w2)) * (1 - hidden1^2)    # (Nx(M1+1)) array
grad3 <- t(hidden2) %*% delH                          # ((M2+1)x1) array
grad2 <- t(hidden1) %*% delHidden2[, 2:(M2+1)]        # ((M1+1)xM2) array
grad1 <- t(X) %*% delHidden1[, 2:(M1+1)]              # ((d+1)xM1) array
# Step 3: gradient descent
w1 <- w1 - eta * grad1
w2 <- w2 - eta * grad2
w3 <- w3 - eta * grad3
}
# calculate Eout from the test set
hidden1 <- tanh(Xtest %*% w1)
hidden1 <- cbind(rep(1, nrow(hidden1)), hidden1)          # (Nx(M1+1)) array
hidden2 <- tanh(hidden1 %*% w2)
hidden2 <- cbind(rep(1, nrow(hidden2)), hidden2)          # (Nx(M2+1)) array
H <- tanh(hidden2 %*% w3)                                 # hypothesis (Nx1) array
ypred <- sign(H)
ypred[ypred == 0] <- 1
Eout <- mean(as.numeric(ypred != ytest))
Eout_arr <- c(Eout_arr, Eout)
}
proc.time() - ptm  # >  seconds
cat('Averaged Eout:', mean(Eout_arr))
# Training parameters
M1 <- 8                                           # d-M1-M2-1 ANN
M2 <- 3
r <- .1
eta <- .01                                        # d-M-1 ANN, eta in c(.001,.01,.1,1,10)
# Backprop algorithm
Eout_arr <- c()
ptm <- proc.time()
for (exp in 1:50) {
# weights randomly (uniformly) initialized
w1 <- runif((d+1)*M1, -r, r)
w1 <- matrix(w1, (d+1), M1)                         # ((d+1)xM1) array
w2 <- runif((M1+1)*M2, -r, r)
w2 <- matrix(w2, (M1+1), M2)                        # ((M1+1)xM2) array
w3 <- runif((M2+1)*1, -r, r)
w3 <- matrix(w3, (M2+1), 1)                         # ((M2+1)x1) array
for (iter in 1:5e4) {
# Step 1: feedforward
hidden1 <- tanh(X %*% w1)
hidden1 <- cbind(rep(1, N), hidden1)                  # (Nx(M1+1)) array
hidden2 <- tanh(hidden1 %*% w2)
hidden2 <- cbind(rep(1, N), hidden2)                  # (Nx(M2+1)) array
H <- tanh(hidden2 %*% w3)                             # hypothesis (Nx1) array
# Step 2: backprop and get the gradient of SQUARED ERROR
delH <- -2 * (y - H) * (1 - H^2)                      # (Nx1) array
delHidden2 <- (delH %*% t(w3)) * (1 - hidden2^2)      # (Nx(M2+1)) array
delHidden1 <- (delHidden2[, 2:(M2+1)] %*% t(w2)) * (1 - hidden1^2)    # (Nx(M1+1)) array
grad3 <- t(hidden2) %*% delH                          # ((M2+1)x1) array
grad2 <- t(hidden1) %*% delHidden2[, 2:(M2+1)]        # ((M1+1)xM2) array
grad1 <- t(X) %*% delHidden1[, 2:(M1+1)]              # ((d+1)xM1) array
# Step 3: gradient descent
w1 <- w1 - eta * grad1
w2 <- w2 - eta * grad2
w3 <- w3 - eta * grad3
}
# calculate Eout from the test set
hidden1 <- tanh(Xtest %*% w1)
hidden1 <- cbind(rep(1, nrow(hidden1)), hidden1)          # (Nx(M1+1)) array
hidden2 <- tanh(hidden1 %*% w2)
hidden2 <- cbind(rep(1, nrow(hidden2)), hidden2)          # (Nx(M2+1)) array
H <- tanh(hidden2 %*% w3)                                 # hypothesis (Nx1) array
ypred <- sign(H)
ypred[ypred == 0] <- 1
Eout <- mean(as.numeric(ypred != ytest))
Eout_arr <- c(Eout_arr, Eout)
}
proc.time() - ptm  # >  seconds
cat('Averaged Eout:', mean(Eout_arr))
M <- 3                                            # d-M-1 ANN, M in c(1,6,11,16,21)
r <- .1
eta <- .01                                        # d-M-1 ANN, eta in c(.001,.01,.1,1,10)
# Eout avg = 0.0759 for M = 3, r = .1, eta = .1
# Eout avg = 0.0365 for M = 3, r = .1, eta = .01
# Eout avg = 0.0360 for M = 3, r = .1, eta = .001
# Backprop algorithm
Eout_arr <- c()
ptm <- proc.time()
for (exp in 1:50) {
# weights randomly (uniformly) initialized
w1 <- runif((d+1)*M, -r, r)
w1 <- matrix(w1, (d+1), M)                        # ((d+1)xM) array
w2 <- runif((M+1)*1, -r, r)
w2 <- matrix(w2, (M+1), 1)                        # ((M+1)x1) array
for (iter in 1:5e4) {
# Step 1: feedforward
hidden <- tanh(X %*% w1)
hidden <- cbind(rep(1, N), hidden)                # (Nx(M+1)) array
H <- tanh(hidden %*% w2)                          # hypothesis (Nx1) array
# Step 2: backprop and get the gradient of SQUARED ERROR
delH <- -2 * (y - H) * (1 - H^2)                  # (Nx1) array
grad2 <- t(hidden) %*% delH                       # ((M+1)x1) array
delHidden <- (delH %*% t(w2)) * (1 - hidden^2)
# (Nx(M+1)) array, and
# tanh'(s) = 1 - (tanh(s))^2
# delHidden = delHidden[, 2:(M+1)]
grad1 <- t(X) %*% delHidden[, 2:(M+1)]            # ((d+1)xM) array
# Step 3: gradient descent
w1 <- w1 - eta * grad1
w2 <- w2 - eta * grad2
}
# calculate Eout from the test set
hidden <- tanh(Xtest %*% w1)
hidden <- cbind(rep(1, nrow(hidden)), hidden)         # (Nx(M+1)) array
H <- tanh(hidden %*% w2)                              # hypothesis (Nx1) array
ypred <- sign(H)
ypred[ypred == 0] <- 1
Eout <- mean(as.numeric(ypred != ytest))
Eout_arr <- c(Eout_arr, Eout)
}
proc.time() - ptm  # > 913.747 seconds
cat('Averaged Eout:', mean(Eout_arr))
M <- 3                                            # d-M-1 ANN, M in c(1,6,11,16,21)
r <- .1
eta <- .01                                        # d-M-1 ANN, eta in c(.001,.01,.1,1,10)
# Eout avg = 0.0759 for M = 3, r = .1, eta = .1
# Eout avg = 0.0365 for M = 3, r = .1, eta = .01
# Eout avg = 0.0360 for M = 3, r = .1, eta = .001
# Backprop algorithm
Eout_arr <- c()
ptm <- proc.time()
for (exp in 1:500) {
# weights randomly (uniformly) initialized
w1 <- runif((d+1)*M, -r, r)
w1 <- matrix(w1, (d+1), M)                        # ((d+1)xM) array
w2 <- runif((M+1)*1, -r, r)
w2 <- matrix(w2, (M+1), 1)                        # ((M+1)x1) array
for (iter in 1:5e4) {
# Step 1: feedforward
hidden <- tanh(X %*% w1)
hidden <- cbind(rep(1, N), hidden)                # (Nx(M+1)) array
H <- tanh(hidden %*% w2)                          # hypothesis (Nx1) array
# Step 2: backprop and get the gradient of SQUARED ERROR
delH <- -2 * (y - H) * (1 - H^2)                  # (Nx1) array
grad2 <- t(hidden) %*% delH                       # ((M+1)x1) array
delHidden <- (delH %*% t(w2)) * (1 - hidden^2)
# (Nx(M+1)) array, and
# tanh'(s) = 1 - (tanh(s))^2
# delHidden = delHidden[, 2:(M+1)]
grad1 <- t(X) %*% delHidden[, 2:(M+1)]            # ((d+1)xM) array
# Step 3: gradient descent
w1 <- w1 - eta * grad1
w2 <- w2 - eta * grad2
}
# calculate Eout from the test set
hidden <- tanh(Xtest %*% w1)
hidden <- cbind(rep(1, nrow(hidden)), hidden)         # (Nx(M+1)) array
H <- tanh(hidden %*% w2)                              # hypothesis (Nx1) array
ypred <- sign(H)
ypred[ypred == 0] <- 1
Eout <- mean(as.numeric(ypred != ytest))
Eout_arr <- c(Eout_arr, Eout)
}
proc.time() - ptm  # > 913.747 seconds
cat('Averaged Eout:', mean(Eout_arr))
