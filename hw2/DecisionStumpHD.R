## Decision stump for multi-dimensional data
## h(x_i) = s * sign(x_i-theta) for each dimension
## One-dimensional data are generated by
##   (a) Generate x from the d dimension in the training set
##   (b) Generate y by f(x)=sign(x) + noise, the noise flips the result with 20% probability

# Import the data
# Training set import
datTrain <- read.table("http://d396qusza40orc.cloudfront.net/ntumlone%2Fhw2%2Fhw2_train.dat",
                       header = TRUE)
# Test set import
datTest <- read.table("http://d396qusza40orc.cloudfront.net/ntumlone%2Fhw2%2Fhw2_test.dat",
                      header = TRUE)

# Settings
N <- length(datTrain[,1])                   # training set size
errdim <- c()                               # avg E_in of each dimension
params <- c()                               # (s,theta): best decision stump of dimension d
bestErr <- rep(1, (length(datTrain)-1))     # Initial value of best E_in of each dimension
# errtest <- c()   # E_out: error of test set
# params <- c()    # (s,theta) of each repetitive training trial

for (d in 1:(length(datTrain)-1)) {
  errarr <- c()                             # E_in of each repetitive training trial
  
  # Run the learning for 5000 times for the data of each dimension
  for (k in 1:5000) {
    # Generate the random training data from pattern (a) and (b)
    x <- datTrain[, d]
    noise <- numeric(N)
    noise[sample(N, N*.2)] <- noise[sample(N, N*.2)] + 1
    y <- sign(x)
    y[which(noise != 0)] <- y[which(noise != 0)] * -1
    
    # Initialize the thresholds for each dichotomy
    theta <- numeric(N)
    datap <- c(min(x)-1, sort(x))
    for (i in 1:N) {
      theta[i] <- (datap[i] + datap[i+1])/ 2
    }
    
    err <- 1    # initial error
    for (i in 1:N) {
      ## look up each hypothesis and keep the smallest error and params
      stmp <- 1
      h <- stmp * sign(x-theta[i])
      errtmp <- mean(as.numeric(h != y))
      if (errtmp < err) {
        err <- errtmp
        s <- stmp
        thres <- theta[i]
      }
      stmp <- -1
      h <- stmp * sign(x-theta[i])
      errtmp <- mean(as.numeric(h != y))
      if (errtmp < err) {
        err <- errtmp
        s <- stmp
        thres <- theta[i]
      }
    }
    errarr <- c(errarr, err)
    
    # save best E_in and params to dimension d
    if (err < bestErr[d]) {
      bestErr[d] <- err
      bestParam <- c(s,thres)
    }
  }
  errdim <- c(errdim, mean(errarr))
  params <- rbind(params, bestParam)
}


## Examine E_out from the test set
bestDimension <- which(bestErr == min(bestErr))[1]
bbParam <- params[bestDimension, ]
N <- length(datTest[,1])
x <- datTest[, bestDimension]
noise <- numeric(N)
noise[sample(N, N*.2)] <- noise[sample(N, N*.2)] + 1
y <- sign(x)
y[which(noise != 0)] <- y[which(noise != 0)] * -1
h <- bbParam[1] * sign(x - bbParam[2])
E_out <- mean(as.numeric(h != y))


# Plot the final one of the data for vision
plot(which(y==1), x[y==1], pch = 1, col = "blue", ylim = c(-10,10), 
     xlab = "training example m", ylab = "x in [-1,1]")
points(which(y==-1), x[y==-1], pch = 4, col = "red")
